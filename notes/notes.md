
## Task
### High Level
* Write a project proposal.
* Add a misogyny detector to live system of chat messaging platform.
* Misogyny detector produces a score for every message.
* Messages should be analyzed and automatically redacted in real time.
* Make assumptions about the technical and non-technical requirements of the client and include these assumptions in your report. 

### Low Level
* Divide the project into:
    * Work streams
    * Work packages
    * Deliverables
    * Time/human resource estimates
* Answer the following questions focusing primarily on providing detail on the data and AI strategy:
    * What are your data needs?
        * Client will provide you with the unlabelled message data.
        * Use of a misogyny knowledge base (KB) that your consultancy created in a previous project.
            * KB would contain a taxonomy of misogynistic terms and expressions, annotated with severities.
    * What requirements do you need to collect from the client?
    * What experiments do you need to conduct?
    * Are there any preprocessing considerations for the client’s data?
    * What are potential AI solutions for the misogyny detector and what are the tradeoffs?
    * Select the AI approach you expect to work best in practice and assume for the rest of the proposal (e.g. the system design, resource estimates) that this is going to be the solution you will be implementing.
* System Design:
    * Sketch out high-level system architecture.
    * Assume it will be part of the client’s live system.
        * Containerised services???
    * Make assumptions about how the client’s system is designed.



### REQS
* Social chat messaging platform.
* Billions of messages per day.


## TASK WBS
1. Define Problem
    * Misogyny in chat messages...
    * Brand reputation
2. Define Main Objective
    * Add a misogyny detector to live system of chat messaging platform.
    * Misogyny detector produces a score for every message.
    * Messages should be analyzed and automatically redacted in real time.
3. Define Challenges
    * Tradeoff: 
        * User privacy if going through their data, vs. 
        * Customer churn if allow toxic behaviour
    * Technical:
        * Large volume of streamed data: solutions must be light and fast.
        * Embedded solution: can't impact the rest of the client's system. 
        * Ambiguity in natural language (when is it misogyny and when is it a woman being sarcastic?) 
    * Ethical:
        * What does the client consider misogyny?
            * Damning/cursing
            * Discredit
            * Sexual Harassment and Threats of Violence
            * Stereotype and Objectification
            * Dominance
            * Derailing
            * Discrediting
                * source: https://arrow.tudublin.ie/cgi/viewcontent.cgi?article=1007&context=ittscicon
                * source: https://www.researchgate.net/publication/355583667_AI-based_Misogyny_Detection_from_Arabic_Levantine_Twitter_Tweets
        * Who draws the line between outright misogyny and objective/mature conversations?
            * i.e. an aggressive troll vs a couple of friends talking about misogyny.
        * If a comment comes from a woman (e.g. a religious conservative), would it be considered misogynous?
            * How do we know if it's not a fake profile?  
4. Define Requirements
    * Product:
        * Does it enable Group chats?
            * Assume it does.
            * #DOUBT: Would misogyny be greater in group chats than in 1:1 messages?
        * Media files?
            * Images
            * Audio
    * Data:
        * How many languages?
        * Encryption?
            * End-2-End would pose sever difficulties.
            * Assume it's not E2E.
        * Anonymisation?
            * #REQ: Provide users' sex, if available in users' profiles.
        * Volume?
            * Number of MAU
            * Avg. number of messages per user per day
            * Avg. number of conversations per user per day
                * Defined as with how many other users does a a user interact in a day.
            * Avg. time between messages sent by a same user
            * Avg. response time from other user in conversation
            * Avg. number of active groups per day
                * Same statistics as for 1:1 conversations...
            * Avg. latency from message sent to message received between users currently logged in in the platform.
                * #REQ: What would be the acceptable SLO for any increased latency generated by the misogyny detector?
        * Do you track sessions?
            * How do you define a session?
        * Do you have a system for users to flag when they receive misogynistic content?
            * Can you associate a flag to a specific conversation?
    * System:
        * ...
5. Assumptions of Client System 
6. Data Prepocessing:
    * Decryption
    * Anonymisation
    * Language detection
    * Session/Conversation ID:
        * Associating conversations / threads of related messages in a 1:1 or group interaction.
        * The misogyny could be covert and might only be detectable across a thread of messages.
    * Processing of emoji and other non-alphabetic symbols
        * Emoji and symbol dictionary
    * Accronym expansion
    * Typos handling
    * Handling missing punctuation
    * Sentence and clause parsing
    * Word/phrase tokenization
    * Normalization
        * KB-driven
        * Stemming
        * Lemmatization
7. Modelling:
    * String matching
    * regex pattern extraction
    * Produce labelled data programatically using string-matching and regex models
    * Train misogyny domain-specific NER models
    * Train misogyny domain-specific classifiers
        * Binary classifiers: Does message have misogynistic content?
        * Multi-class classifiers:
            * Damining
            * Derailing
            * Discredit
            * Sexual Harassment
            * Stereotyping and Objectification
            * Threat of Violence
            * None
    * Fine-tune transformers (e.g. Bert)
7. Trade-offs
    * Accuracy
    * Data overheads
    * Training overheads
    * Production overheads
8. Define Experiments
    * Misogyny greater in group chats?
    * Multiple technics/models.
    * Academic benchmarks.
    * Feedback from client SME and T&S.
    * Beta test with cohort of users to flag when unredacted misogynistic messages filter through (i.e. false negatives).
    * A/B test (if a client-side flagging system is available)
9. Model Selection
    * Justify
    * Value
        * Iteration Velocity
        * How soon can it be put into production?
        * How soon can you quantify results?

10. High-level system architecture
    * Users that enable End-to-End encryption can have a minimal client-side misogyny detection service.
        * Primarily word/phrase matching and regex running straigh from the application on users mobile devices or browsers.
11. Low-level system design
12. Work Breakdown Structure
    * Work streams
    * Work packages
    * Milestones
    * Deliverables
    * Resourcing

### Scratpad
* Handling/flagging of other toxic behaviour
* Trust & Safety on the client's side to own policies and resolution of ambiguities?    
* End-2End encryption:
    * Cant' have it since we would be unable to apply the verification.
        * Middle-tier services wouldn't be able to read the data.
    * WhatsApp by default has E2E
        * Data is stored until the recipient receives the message.
    * In Facebook Messenger you have to turn it on.
        * Unless turned on, FB stores the data sent.
* Queues: https://betterprogramming.pub/introduction-to-message-queue-with-rabbitmq-python-639e397cb668
* PubSubs: -> for Groups
* Spark streaming
